name: Nightly Scrape (11pm IST)

on:
  schedule:
    - cron: '30 17 * * *'
  workflow_dispatch: {}

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; else pip install requests beautifulsoup4 google-generativeai firebase-admin python-dotenv gspread oauth2client pandas; fi

      - name: Write Firebase credentials file
        env:
          FIREBASE_CREDENTIALS_JSON: ${{ secrets.FIREBASE_CREDENTIALS_JSON }}
        run: |
          echo "$FIREBASE_CREDENTIALS_JSON" > service-account.json
          echo "FIREBASE_CREDENTIALS_JSON written to service-account.json"

      - name: Write Google Sheets credentials file
        env:
          GOOGLE_SHEETS_CREDENTIALS_JSON: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS_JSON }}
        run: |
          echo "$GOOGLE_SHEETS_CREDENTIALS_JSON" > credentials.json
          echo "Google Sheets creds written to credentials.json"

      - name: Run scraper
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GMAIL_FROM_EMAIL: ${{ secrets.GMAIL_FROM_EMAIL }}
          GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          FIREBASE_CREDENTIALS_PATH: service-account.json
        run: |
          python - <<'PY'
          import os
          import sys
          # ensure repo root in path
          sys.path.insert(0, os.getcwd())
          # prefer scripts/ import style
          try:
              from scripts.enhanced_scraper_v2 import scrape_all_teams
          except Exception as e:
              print('Import fallback to run file directly:', e)
              os.system('python scripts/enhanced_scraper_v2.py')
          else:
              scrape_all_teams()
          PY
